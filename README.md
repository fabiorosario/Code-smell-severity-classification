Detecting code smells through machine learning (ML) poses challenges due to its unbalanced nature and potential interpretation bias. While previous studies focused on severity tended to categorize code smell´s specific types, this research aims to detect and classify code-smell severity in a single dataset containing instances of smell´s four distinct types: God-class, Data-Class, Feature-Envy, and Long-Method. This study explores also the impact of applying data preprocessing, feature selection techniques, and ensemble methods to enhance ML models for this purpose. The evaluation of various machine-learning models on a merged dataset reveals that Linear Discriminant Analysis (LDA) shows improvements ranging from 13.38\% to 26.76\% over different models. However, a combination of dataset standardization techniques, ensemble methods, and Chi-square outperforms LDA, achieving 81.04\% and 81.41\% accuracy in the XGBoost and CatBoost models. Additionally, the CatBoost algorithm attains the highest accuracy at 80.67\% even without data preprocessing. Comparatively with the state of the art, the results obtained by the proposed approach in detecting the severity of code smells suggest the need for improvements in approaches and techniques to enhance the effectiveness and reliability of models in real-world scenarios.

To make it easier to link each file in this repository, I describe them below:

1)	norm x esc-severity.ipynb (Comparison of Normalization and Standardization Techniques)
To determine the most effective technique for scaling the combined dataset, experiments were carried out using Normalization and Standardization techniques. The results presented in Excel spreadsheet “norm x esc-severity.xlsx” reveal that Standardization resulted in improvements in indices, ranging from 6.69% (NB_Bernoulli) to 22.68% (LR). Standardization reached its highest accuracy in the CatBoost model, recording 80.67%. In contrast, Normalization provided improvements only in the LR (0.37%) and KNN (4.09%) algorithms, while the remaining algorithms gained a draw (NB_BernoulliNB) or a drop in accuracy compared to the merged dataset, with DT having the biggest drop (-8,18%). It is worth highlighting that the Tree-based models (DT, RF, XGBoost, and CatBoost) obtained similar results with and without the application of data standardization. These results highlight the relevance of Standardization and specialization methods to improve ML models, while Normalization, in most cases, does not promote significant improvements and often harms ML models. 

2)	lda x chisquare-severity.ipynb (Comparison of LDA and Chi-Square Feature Selection Techniques)
The results obtained in the experiments in this subsection aim to evaluate the more detailed resource selection technique for the proposed approach. The results presented in Excel spreadsheet “lda x chisquare-severity.xlsx”  indicate that LDA showed improvements in values compared to the merged data set, ranging from 13.38% (NB_Bernoulli) to 26.76% (RL). However, these promising results did not surpass the effectiveness of the combination of dataset  standardization techniques, ensemble methods, and Chi-square, which achieved values of 81.04% and 81.41% in the XGBoost and CatBoost models, respectively. Furthermore, the CatBoost algorithm achieved the highest accuracy with 80.67%, even without directly applying the feature selection technique. Therefore, after validating the model with 5-fold cross-validation, the Chi-square feature selection technique was chosen to compose our approach for the detection and classification of code smell severity.

3) code smell detection.ipynb (Comparison of machine learning techniques for code smell detector)
The most appropriate model was chosen for each stage of our approach, after configuring hyperparameters, validating the models, and analyzing the performance improvements of each ensemble method using data scaling and feature selection techniques, according to the results presented in the Excel spreadsheet “code smell detection.xlsx”. In our approach, the first step consists of a code smell detection module composed of the CatBoost ensemble method, standardization, and Chi-square.

4) code smell detection and severity classification.ipynb
Python script for the code smell detector and classifier used in Section 5.2, with the results of the studies experiments. Our approach for detecting and classifying code smell severity included two steps. The first step aims to detect code smells from instances with severity, using the Standardization data preprocessing technique, Chi-Square for feature selection, and the CatBoost ensemble method to binary identify the presence or absence of code smell. The following step removes the negative instances and classifies the code smell severity of the positive instances, using the standardization, chi-square, and XGBoost. This approach obtained 85% accuracy, as described in Table 5 of the article.

5) Analysis for imputation of missing values.ipynb. Python script to analyze attributes with missing values ​​to determine the best strategy for filling in their values.

6) Outliers Analysis.ipynb. Python script to analyze outliers of attributes.

7) merged dataset_FE_LM_GC_DC missing values.csv. Combined dataset with missing values, i.e., method metrics (NOP, CC, ATFD, FDP, CM, MAXNESTING, LOC, CYCLO, NMCS, NOLV, MaMCL, NOAV, LAA, FANOUT, CFNAMM, ATLD, CLNAMM, CINT, MeMCL, CDISP) in the GC and DC instances.

8) merged dataset_FE_LM_GC_DC.csv. Combined dataset with missing values ​​filled in by using the mode value because the sample values for each attribute do not follow a normal distribution, according to histograms generated in the Analysis for imputation of missing values.ipynb.
   

