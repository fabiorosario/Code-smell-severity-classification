Detecting code smells through machine learning (ML) poses challenges due to its unbalanced nature and potential interpretation bias. While previous studies focused on severity tended to categorize code smell´s specific types, this research aims to detect and classify code-smell severity in a single dataset containing instances of smell´s four distinct types: God-class, Data-Class, Feature-Envy, and Long-Method. This study explores also the impact of applying data preprocessing, feature selection techniques, and ensemble methods to enhance ML models for this purpose. The evaluation of various machine-learning models on a merged dataset reveals that Linear Discriminant Analysis (LDA) shows improvements ranging from 13.38\% to 26.76\% over different models. However, a combination of dataset standardization techniques, ensemble methods, and Chi-square outperforms LDA, achieving 81.04\% and 81.41\% accuracy in the XGBoost and CatBoost models. Additionally, the CatBoost algorithm attains the highest accuracy at 80.67\% even without data preprocessing. Comparatively with the state of the art, the results obtained by the proposed approach in detecting the severity of code smells suggest the need for improvements in approaches and techniques to enhance the effectiveness and reliability of models in real-world scenarios.
